<h1 align="center">ğŸ§  URL Scrapability Tester</h1>

<p align="center">
  <b>Smart, automated analyzer that checks if a website is scrapable â€” safely and efficiently.</b><br>
  Built with <a href="https://playwright.dev/python/">Playwright</a> to detect bot protection, rendering complexity, and more.
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.9%2B-blue?logo=python" alt="Python">
  <img src="https://img.shields.io/badge/Playwright-Automation-success?logo=microsoft-edge" alt="Playwright">
  <img src="https://img.shields.io/badge/License-MIT-green" alt="License">
  <img src="https://img.shields.io/badge/Status-Stable-brightgreen" alt="Status">
</p>

---

## âš™ï¸ Overview

The **URL Scrapability Tester** is an automated utility that evaluates whether a website can be scraped effectively.  
It performs **ethical, pre-scraping diagnostics** to determine if a target site allows or blocks automated requests â€” without breaking rules.

### ğŸ§© Key Capabilities
- âœ… Checks **robots.txt** permissions  
- âš™ï¸ Detects **JavaScript rendering** and **login barriers**  
- ğŸ§  Identifies **frontend frameworks** (React, Next.js, WordPress, etc.)  
- ğŸ›¡ï¸ Scans for **bot detection systems** (reCAPTCHA, Cloudflare, DataDome, etc.)  
- ğŸ“¸ Captures **full-page screenshots** for reports  
- ğŸ”¢ Generates a **Scrapability Score (0â€“100)** with difficulty levels  

---

## ğŸ§  Scrapability Score System

| Factor | Impact | Penalty |
|--------|---------|----------|
| `robots.txt` disallows scraping | Medium | âˆ’15 |
| Bot protection (CAPTCHA, CF, etc.) | Critical | âˆ’30 |
| JavaScript rendering required | Medium | âˆ’12 |
| Login required | High | âˆ’20 |
| Heavy frontend tech (React, Shopify, etc.) | Variable | âˆ’5 to âˆ’20 |

**Score Range & Difficulty:**
| Score | Level |
|--------|--------|
| 80â€“100 | ğŸŸ¢ Easy |
| 50â€“79  | ğŸŸ¡ Moderate |
| 0â€“49   | ğŸ”´ Hard |

---

## ğŸ–¼ï¸ Example Output = examples-output.txt

